{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization, Stemming, and Lemmatization with SpaCy Library.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIugsrpv1eGt",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to SpaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQsydZzj1jZc",
        "colab_type": "text"
      },
      "source": [
        "The spaCy library is one of the most popular NLP libraries along with NLTK. The basic difference between the two libraries is the fact that NLTK contains a wide variety of algorithms to solve one problem whereas spaCy contains only one, but the best algorithm to solve a problem.\n",
        "\n",
        "NLTK was released back in 2001 while spaCy is relatively new and was developed in 2015. In this series of articles on NLP, we will mostly be dealing with spaCy, owing to its state of the art nature. However, we will also touch NLTK when it is easier to perform a task using NLTK rather than spaCy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoHNXsaA1hMW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "0bb9d165-d414-432f-d02c-27141bb061f7"
      },
      "source": [
        "!pip install -U spacy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.38.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (46.0.0)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfcwxIsM1wkK",
        "colab_type": "text"
      },
      "source": [
        "Once you download and install spaCy, the next step is to download the language model. We will be using the English language model. The language model is used to perform a variety of NLP tasks, which we will see in a later section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUDeo4O51pp6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "c30aaa31-0334-4eb9-ce91-151ac929524a"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNTontBP12C9",
        "colab_type": "text"
      },
      "source": [
        "## Basic Functionality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWKJwCqM1zpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "sp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2MYBwHt19Gg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fd6f15f-3b0e-45cc-a806-166d1a53799d"
      },
      "source": [
        "sp"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7f587c4d8a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5wlXkzJ2EQH",
        "colab_type": "text"
      },
      "source": [
        "Let's now create a small document using this model. A document can be a sentence or a group of sentences and can have unlimited length. The following script creates a simple spaCy document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN_P96p01-Ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = sp(u'Manchester United is looking to sign a forward for $90 million')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k63Fi0X2LOm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c9fe6c1-9cd3-48fb-b83d-7311080de04f"
      },
      "source": [
        "sentence"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Manchester United is looking to sign a forward for $90 million"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hUkV-po2TNT",
        "colab_type": "text"
      },
      "source": [
        "SpaCy automatically breaks your document into tokens when a document is created using the model.\n",
        "\n",
        "A token simply refers to an individual part of a sentence having some semantic value. Let's see what tokens we have in our document:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ6aG9pd2Myc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "eb747eda-a518-4c47-c39c-874f6452ecea"
      },
      "source": [
        "for word in sentence:\n",
        "    print(word.text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Manchester\n",
            "United\n",
            "is\n",
            "looking\n",
            "to\n",
            "sign\n",
            "a\n",
            "forward\n",
            "for\n",
            "$\n",
            "90\n",
            "million\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5HQRA9v6MEG",
        "colab_type": "text"
      },
      "source": [
        "You can see we have the following tokens in our document. We can also see the parts of speech of each of these tokens using the .pos_ attribute shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlaBfnnu6Ebc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c6147a50-ea14-400b-b52a-47b037830479"
      },
      "source": [
        "for word in sentence:\n",
        "    print(word.text,  word.pos_)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Manchester PROPN\n",
            "United PROPN\n",
            "is AUX\n",
            "looking VERB\n",
            "to PART\n",
            "sign VERB\n",
            "a DET\n",
            "forward NOUN\n",
            "for ADP\n",
            "$ SYM\n",
            "90 NUM\n",
            "million NUM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeaIb1gx6hyu",
        "colab_type": "text"
      },
      "source": [
        "Finally, in addition to the parts of speech, we can also see the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc_0sInl2a1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence2 = sp(u\"Manchester United isn't looking to sign any forward.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHzQnoKe6kmV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c6bd4ddf-aaa5-429d-e590-084098501889"
      },
      "source": [
        "for word in sentence2:\n",
        "    print(word.text,  word.pos_, word.dep_)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Manchester PROPN compound\n",
            "United PROPN nsubj\n",
            "is AUX aux\n",
            "n't PART neg\n",
            "looking VERB ROOT\n",
            "to PART aux\n",
            "sign VERB xcomp\n",
            "any DET advmod\n",
            "forward ADV advmod\n",
            ". PUNCT punct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CagonS-Z6vHf",
        "colab_type": "text"
      },
      "source": [
        "From the output, you can see that spaCy is intelligent enough to find the dependency between the tokens, for instance in the sentence we had a word is'nt. The depenency parser has broken it down to two words and specifies that the n't is actually negation of the previous word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxi1mzlE7DqE",
        "colab_type": "text"
      },
      "source": [
        "In addition to printing the words, you can also print sentences from a document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uniStk0q6nc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document = sp(u'Hello from Stackabuse. The site with the best Python Tutorials. What are you looking for?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMepNSgp7GyD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1a18072d-86e3-4cb0-d8b9-6675a898d2b9"
      },
      "source": [
        "for sentence in document.sents:\n",
        "    print(sentence)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello from Stackabuse.\n",
            "The site with the best Python Tutorials.\n",
            "What are you looking for?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQXkVHfj7JEK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b90d5ad-f0d6-441a-8db6-0453e43c02b5"
      },
      "source": [
        "document[4].is_sent_start"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0NmodXP7dU_",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6YKZ8Ne8BT0",
        "colab_type": "text"
      },
      "source": [
        "As explained earlier, tokenization is the process of breaking a document down into words, punctuation marks, numeric digits, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9Vwqpfz7YJo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b69f7cb-eb79-4c0f-f272-145b36ce38c0"
      },
      "source": [
        "sentence3 = sp(u'\"They\\'re leaving U.K. for U.S.A.\"')\n",
        "print(sentence3)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"They're leaving U.K. for U.S.A.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec5iT3oo8HPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "353313eb-59e8-43c2-cfa4-9fb60d53c8b0"
      },
      "source": [
        "for word in sentence3:\n",
        "    print(word.text)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"\n",
            "They\n",
            "'re\n",
            "leaving\n",
            "U.K.\n",
            "for\n",
            "U.S.A.\n",
            "\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9prXcAap8QOl",
        "colab_type": "text"
      },
      "source": [
        "In the output, you can see that spaCy has tokenized the starting and ending double quotes. However, it is intelligent enough, not to tokenize the punctuation dot used between the abbreviations such as U.K. and U.S.A."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMfmlrXo8KXc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf5bb0fa-4aa2-4521-d1c6-9128d415de1c"
      },
      "source": [
        "sentence4 = sp(u\"Hello, I am non-vegetarian, email me the menu at abc-xyz@gmai.com\")\n",
        "print(sentence4)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello, I am non-vegetarian, email me the menu at abc-xyz@gmai.com\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYQ_mtOI8Tu2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "512ff364-ffba-419f-aef6-7eece39bc160"
      },
      "source": [
        "for word in sentence4:\n",
        "    print(word.text)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello\n",
            ",\n",
            "I\n",
            "am\n",
            "non\n",
            "-\n",
            "vegetarian\n",
            ",\n",
            "email\n",
            "me\n",
            "the\n",
            "menu\n",
            "at\n",
            "abc-xyz@gmai.com\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXCXbnXX8kFk",
        "colab_type": "text"
      },
      "source": [
        "It is evident from the output that spaCy was actually able to detect the email and it did not tokenize it despite having a \"-\". On the other hand, the word \"non-vegetarian\" was tokenized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L268B4Eh8Y8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c37cd677-5249-4b5d-b1f1-713b637575b8"
      },
      "source": [
        "len(sentence4)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyHeUY_g8xX9",
        "colab_type": "text"
      },
      "source": [
        "In the output, you will see 14, which is the number of tokens in the sentence4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSvtPMua83dU",
        "colab_type": "text"
      },
      "source": [
        "## Detecting Entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArvTPI099AbO",
        "colab_type": "text"
      },
      "source": [
        "Let's see a simple example of named entity recognition:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eld7dDl_8rls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence5 = sp(u'Manchester United is looking to sign Harry Kane for $90 million')  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9MaqWzB9CyU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "22e93cda-8d54-45b1-f772-4b08d164f8a5"
      },
      "source": [
        "for word in sentence5:\n",
        "    print(word.text)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Manchester\n",
            "United\n",
            "is\n",
            "looking\n",
            "to\n",
            "sign\n",
            "Harry\n",
            "Kane\n",
            "for\n",
            "$\n",
            "90\n",
            "million\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vxEbQnu9Oxk",
        "colab_type": "text"
      },
      "source": [
        "We know that \"Manchester United\" is a single word, therefore it should not be tokenized into two words. Similarly, \"Harry Kane\" is the name of a person, and \"$90 million\" is a currency value. These should not be tokenized either.\n",
        "\n",
        "This is where named entity recognition comes to play. To get the named entities from a document, you have to use the ents attribute. Let's retrieve the named entities from the above sentence. Execute the following script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jMF0DTe9FPu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ebef7500-7212-4e2d-a7d9-970bfb77f146"
      },
      "source": [
        "for entity in sentence5.ents:\n",
        "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Manchester United - PERSON - People, including fictional\n",
            "Harry Kane - PERSON - People, including fictional\n",
            "$90 million - MONEY - Monetary values, including unit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BunXpIfR9huz",
        "colab_type": "text"
      },
      "source": [
        "You can see that spaCy's named entity recognizer has successfully recognized \"Manchester United\" as an organization, \"Harry Kane\" as a person and \"$90 million\" as a currency value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwWP4p789mNE",
        "colab_type": "text"
      },
      "source": [
        "## Detecting Nouns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKHcEM-E9UKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence5 = sp(u'Latest Rumours: Manchester United is looking to sign Harry Kane for $90 million')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrU4HzqZ_A8c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6486bb88-22bd-4406-e1a6-91d5b73fe1fc"
      },
      "source": [
        "for noun in sentence5.noun_chunks:\n",
        "    print(noun.text)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Manchester United\n",
            "Harry Kane\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVxvBnac_NUl",
        "colab_type": "text"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbj1o8pf_T7C",
        "colab_type": "text"
      },
      "source": [
        "Stemming refers to reducing a word to its root form. While performing natural language processing tasks, you will encounter various scenarios where you find different words with the same root. For instance, compute, computer, computing, computed, etc. You may want to reduce the words to their root form for the sake of uniformity. This is where stemming comes in to play."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_VWQTIB_apy",
        "colab_type": "text"
      },
      "source": [
        "There are two types of stemmers in NLTK: Porter Stemmer and Snowball stemmers. Both of them have been implemented using different algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQz2FRXD_c9X",
        "colab_type": "text"
      },
      "source": [
        "## Porter Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js18ke3x_Do5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.stem.porter import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8Qdr_pD_gY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psr59BAI_jwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = ['compute', 'computer', 'computed', 'computing']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7VNkN2z_l6p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cc413a57-376c-40fb-b5ae-a69465b911eb"
      },
      "source": [
        "for token in tokens:\n",
        "    print(token + ' --> ' + stemmer.stem(token))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "compute --> comput\n",
            "computer --> comput\n",
            "computed --> comput\n",
            "computing --> comput\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QdouJ0h_sI0",
        "colab_type": "text"
      },
      "source": [
        "You can see that all the 4 words have been reduced to \"comput\" which actually isn't a word at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EoaWT0j_vNy",
        "colab_type": "text"
      },
      "source": [
        "## Snowball Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33xicJMK_oQA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "aaee9957-04fb-4454-8a88-0c0272ba2ff4"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "tokens = ['compute', 'computer', 'computed', 'computing']\n",
        "\n",
        "for token in tokens:\n",
        "    print(token + ' --> ' + stemmer.stem(token))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "compute --> comput\n",
            "computer --> comput\n",
            "computed --> comput\n",
            "computing --> comput\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsZLjXhc_-2l",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsdXMBS-_3rI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence6 = sp(u'compute computer computed computing')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VJUGotgAHRb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "84aca854-11df-4060-be11-6b5511207a9c"
      },
      "source": [
        "for word in sentence6:\n",
        "    print(word.text,  word.lemma_)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "compute compute\n",
            "computer computer\n",
            "computed compute\n",
            "computing computing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aDCV4nkAQ6f",
        "colab_type": "text"
      },
      "source": [
        "Lemmatization converts words in the second or third forms to their first form variants. Look at the following example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJzdJeI-AJrc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "eb26da9d-c26b-4b0c-f113-8d770d7565fa"
      },
      "source": [
        "sentence7 = sp(u'A letter has been written, asking him to be released')\n",
        "\n",
        "for word in sentence7:\n",
        "    print(word.text + '  ===>', word.lemma_)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A  ===> a\n",
            "letter  ===> letter\n",
            "has  ===> have\n",
            "been  ===> be\n",
            "written  ===> write\n",
            ",  ===> ,\n",
            "asking  ===> ask\n",
            "him  ===> -PRON-\n",
            "to  ===> to\n",
            "be  ===> be\n",
            "released  ===> release\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tcgu0pXzATm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}